---
tags:
  - мм_тема
  - мм_метод
---
[[Математические методы планирования и интерпретации эксперимента]]  
Prev Theme: [[Цифровое сглаживание и дифференцирование]]  
Next Theme: [[Плохая обусловленность и её улучшение]]

---
### Метод М-оценок. Функция Хьюбера
Как упоминалось, МНК чувствителен к аномальным измерениям. Поэтому существуют более **робастные** методы учитывающие это. Например, **метод М-оценок**, в котором производится минимизация не квадрата ошибки, а некоторой функции от него:
$$
J_{\rho}(b) = \sum\limits_{i=1}^n \rho (\epsilon_{i}) = \sum\limits_{i=1}^n \rho (y_{i} - \sum\limits_{j=1}^kx_{ij}b_{j})
$$
В стантартном МНК $\rho(r) = r^2$. Хьюбер предложил функцию $\rho$ вида
$$
\rho(r) = \left\{
\begin{aligned}
& \dfrac{r^2}{2},\ |r| \leq a \\
& a |r| - \dfrac{a^2}{2},\ |r| \geq a
\end{aligned}
\right.
$$
здесь $a$ - параметр, значение которого ($\approx 1.5$) может быть установлено таким образом, чтобы дисперсия оценки была как можно ближе к оптимальной (дисперсии оценки МНК).

Найдём оценку параметров регрессии из минимизации $J_{\rho}(b)$:
$$
\dfrac{\partial J_{\rho}}{\partial b_{j}} = − \sum\limits_{i=1}^n w_{i} \epsilon_{i}x_{ij} = − \sum\limits_{i=1}^n w_{i} \left(y_{i} - \sum\limits_{j=1}^kx_{ij}b_{j}\right)x_{ij}​ = 0,\ j = \overline{1,k}
$$
где вводятся весовые коэффициенты 
$$
w_{i} = \dfrac{\rho'(\epsilon_{i})}{\epsilon_{i}} = \left\{
\begin{aligned}
& 1,\ |\epsilon_{i}| \leq a \\
& \dfrac{a}{|\epsilon_{i}|},\ |\epsilon_{i}| > a
\end{aligned}
\right.
$$
чтобы получить систему уравнений, похожую на [[Свойства оценки МНК|систему нормальных уравнений МНК]]:
$$
X'WXb = X'WY,\ W = \mathrm{diag}(w_{1},\dots,w_{n})
$$
В стандартном МНК матрица $W = I$, однако здесь она не только отличается от единичной, но также зависит от самого вектора $b$. Предлагается ввести итерационный процесс
$$
\begin{aligned}
& b_{(1)} = (X'X)^{-1}X'Y \\
& b_{(k+1)} = (X'W_{(k)}X)^{-1}X'W_{(k)}Y,\quad k = 1,2,\dots
\end{aligned}
$$
который необходимо продолжать до сходимости. Здесь $W_{(k)} = W(b_{(k)})$.

**Замечание:** понятно, что уменьшение чувствительности происходит не бесплатно - теряется оптимальность.