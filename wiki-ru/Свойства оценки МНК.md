---
tags:
  - мм_результат
---
[[Математические методы планирования и интерпретации эксперимента]]
Prev Theme: [[Метод наименьших квадратов (МНК)]]
Next Theme: [[Метод максимального правдоподобия (ММП)]]

---
### Формула
Оценку МНК можно найти, решив систему нормальных уравнений МНК
$$
(X'X)\hat{b} = X'Y
$$
Эта система получается из условия $\dfrac{dJ(b)}{db} = 0$
Если $|X'X| \ne 0$, то оценка МНК (решение [[Постановка задачи классической линейной регрессии (КЛР)|задачи КЛР]]) выражается через $X$ и $Y$:
$$
\hat{b} = (X'X)^{-1}X'Y
$$
### Несмещённость 
Оценка МНК несмещённая
$$
   E(\hat{b}) = b
$$
**Доказательство:**
$$
E(\hat{b}) = E((X'X)^{-1}X'Y) = E((X'X)^{-1}X'(Xb + \epsilon))
$$
Далее мы используем свойство $E(Cx) = C E(x)$, где $C$ - детерминированная матрица, а $x$ - случайный вектор.
$$
E(\hat{b}) = E(b) + (X'X)^{-1}X'E(\epsilon) = b
$$
**Замечание:** Очевидно, что $E(\hat{t}) = E(T\hat{b}) = Tb = t$, то есть оценка параметров $t$ также является несмещённой.
### Дисперсия
Дисперсия оценки МНК:
$$
D(\hat{b}) = \sigma^2 (X'X)^{-1}
$$
**Доказательство:** 
$$
D(\hat{b}) = E (\hat{b} - b)(\hat{b} - b)' = 
$$
$$
=E((X'X)^{-1}X'Y - (X'X)^{-1}X'(Y - \epsilon))(\dots)' = 
$$
$$
=(X'X)^{-1}X'E(\epsilon \epsilon') X (X'X)^{-1} = \sigma^2 (X'X)^{-1}
$$
**Замечание:** Не трудно убедиться, что в случае произвольной линейной оценки вида $\tilde{b} = CY$ дисперсия будет
$$
D(\tilde{b}) = \sigma^2 CC'
$$
### Оптимальность (КЛР)
**Теорема (Гаусс-Марков):** Оценка МНК оптимальна в [[Класс линейных оценок|классе линейных]] по $Y$ оценок
**Доказательство:** Пусть $\tilde{b} = CY$ линейная несмещённая оценка. Несмещённость требует $CX = I$, значит
$$
C = (X'X)^{-1}X'
$$
### Оценка дисперсии
Оценка дисперсии оценки МНК (в случае неизвестной $\sigma^2$):
$$
\hat{D}(\hat{b}) = \hat{\sigma}^2 (X'X)^{-1},\ \hat{\sigma}^2 = \dfrac{J(\hat{b})}{n-k}
$$
**Доказательство:** 
$$
J(b) = (Y - Xb)'(Y - Xb) = (Y - Xb + X\hat{b} - X\hat{b})'(\dots) = 
$$
$$
= J(\hat{b}) + (\hat{b}-b)'X'X(\hat{b}-b) = J(\hat{b}) + Q(\hat{b}-b)
$$
Возьмём матожидание от этого выражения
$$
E(J(b)) = E(\epsilon'\epsilon) = n \sigma^2
$$
$$
E((\hat{b}-b)'X'X(\hat{b}-b)) = E[\mathrm{tr}(X'X)(\hat{b}-b)(\hat{b}-b)']=
$$
$$
= \mathrm{tr}(X'X)(X'X)^{-1}\sigma^2 = k \sigma^2
$$
Где $\text{tr} A$ есть след матрицы $A$. Таким образом:
$$
\sigma^2 = \dfrac{E(J(\hat{b}))}{n-k} = E\left(\dfrac{J(\hat{b})}{n-k}\right)
$$
С другой стороны, если $\hat{\sigma}^2$ - несмещённая оценка дисперсии шума, то 
$$
E(\hat{\sigma}^2) = \sigma^2
$$
Значит
$$
\hat{\sigma}^2 = \dfrac{J(\hat{b})}{n-k}
$$
### Оптимальность (КЛНР)
**Теорема:** Если $\epsilon \sim N(0, \sigma^2 I)$, то оценка МНК [[Эффективная оценка|эффективна]] и оптимальна во всём классе несмещённых оценок.
**Доказательство:** Оценка $\tilde{b}_{э}$ называется эффективной, если $D(\tilde{b}_{э}) = I_{э}^{-1}$, где $I_{э}(b)$ - информационная матрица Фишера:
$$
I_{э}(b) = E\left[\left(\dfrac{\partial \ln L(Y|b)}{\partial b}\right)\left(\dfrac{\partial \ln L(Y|b)}{\partial b}\right)'\right]
$$
$L(Y|b)$ - функция правдоподобия (см. [[Метод максимального правдоподобия (ММП)|ММП]])

В силу теоремы Рао-Крамера, если эффективная оценка существует, то для *любых* несмещённых оценок $\tilde{b}$ справедливо $D(\tilde{b}) \geq D(\tilde{b}_{э})$. То есть, любая эффективная оценка является оптимальной.

Легко показать, что в случае $\epsilon \sim N(0, \sigma^2 I)$ матрица $I_{э} = \dfrac{X'X}{\sigma^2}$, то есть оценка МНК эффективная, а значит оптимальная.
### Основная теорема КЛНР
**Теорема:** Если $\epsilon \sim N(0, \sigma^2 I)$, то 
1. $\hat{b}, J(\hat{b}), Q(\hat{b}-b)$ попарно независимы
2. $\hat{b} \sim N(b, \sigma^2 (X'X)^{-1})$
3. $\dfrac{J(\hat{b})}{\sigma^2} \sim \chi^2(n-k)$
4. $\dfrac{Q(\hat{b}-b)}{\sigma^2} \sim \chi^2(k)$
где $Q(\hat{b}-b) = (\hat{b} - b)'X'X(\hat{b}-b)$, и $\chi^2(k)$ - распределение хи-квадрат с $k$ степенями свободы.
**Доказательство:** без доказательства

**Следствие:** 
1. $\dfrac{\hat{b}_{j}-b_{j}}{\sigma \sqrt{ F_{jj}^{-1} }} \sim N(0,1)$
2. $\dfrac{\hat{b}_{j}-b_{j}}{\hat{\sigma} \sqrt{ F_{jj}^{-1} }} \sim s(n-k)$
3. $\dfrac{Q(\hat{b}-b)}{\hat{\sigma}^2k} = \dfrac{Q(\hat{b}-b)}{J(\hat{b})} \dfrac{n-k}{k} \sim F(k, n-k)$
где $s(n-k)$ - распределение студента, $F(k,n-k)$ распределение Фишера
**Доказательство:** 
1. очевидно
2. 3-й пункт теоремы можно переписать:
$$
\dfrac{J(\hat{b})}{\sigma^2} = (n-k) \dfrac{\hat{\sigma}^2}{\sigma^2} \sim \chi^2(n-k)
$$
Далее делим выражение из пункта 1 Следствия на корень из переписанного выражения, деленное на $(n-k)$ и получаем требуемое.
3. Аналогично пункту 2
### Состоятельность
Сформулируем два набора достаточных условий для состоятельности оценки МНК. Первая теорема используется чаще, т.к. доказать её условие проще, однако, оно довольно сильное и может не выполняться. Тогда можно проверить условия второй теоремы. 

**Теорема 1:** Если матрица $X'X$ удовлетворяет условию сильной регулярности:
$$
\lim_{ n \to \infty } \dfrac{X'X}{n} = A,\ |A| \ne 0
$$
то оценка МНК [[Состоятельная оценка|состоятельна]]
**Доказательство:** В силу несмещённости оценки МНК мы можем использовать неравенство Чебышёва, справедливое для любой случайной величины:
$$
P(|\hat{b}-b| > \epsilon) \leq \dfrac{D(\hat{b})}{\epsilon^2}
$$
Таким образом, если $\lim_{ n \to \infty }D(\hat{b}) = 0$, то оценка МНК состоятельна и теорема доказана. Перепишем условие сильной регулярности:
$$
\lim_{ n \to \infty } (X'X)^{-1}n = A^{-1}
$$
Значит $\lim_{ n \to \infty } (X'X)^{-1} = \lim_{ n \to \infty }D(\hat{b}) = 0$, теорема доказана.

**Теорема 2:** Оценка МНК состоятельна, если выполнены условия:
1. $\lim\limits_{ n \to \infty } \sum\limits_{t=1}^n x_{tj}^2 = \infty,\ \forall j = \overline{1,k}$
2. $R_{ij} = \dfrac{\sum\limits_{t=1}^n x_{ti}x_{tj}}{\sqrt{ \sum\limits_{t=1}^n x_{ti}^2 \sum\limits_{t=1}^n x_{tj}^2 }} \to R,\ |R| \ne 0,\ \forall i,j = \overline{1,k}$
**Доказательство:** без доказательства
### Асимптотическая нормальность
**Теорема:** Пусть 
1. выполнены условия Теоремы 2 о состоятельности оценки МНК (см прошлый пункт)
2. $\epsilon_{i}$ независимы и одинаково распределены
3. $\dfrac{\underset{t}{\mathrm{max}}\ x_{tj}^2}{\sum\limits_{t=1}^{n}x_{tj}^2} \to 0,\ n \to \infty, \forall j = \overline{1,k}$
тогда оценка МНК асимптотически нормально распределена, то есть $\hat{b} \sim N(b, \sigma^2(X'X)^{-1})$
**Доказательство:** без доказательства